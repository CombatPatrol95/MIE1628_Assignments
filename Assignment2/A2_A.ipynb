{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XhLQkdOGLqF"
      },
      "source": [
        "### Part A Questions 1~4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p6O6vhWGGPf",
        "outputId": "61698b35-9536-4105-b3b5-fece1cf60b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=ead5820f072ac3eefbcb3c1bc1bca530cc5dd97f3b1d73aed23d19e9de54acd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n",
            "Odd numbers count: 496\n",
            "Even numbers count: 514\n",
            "Sales 3488491\n",
            "Research 3328284\n",
            "Developer 3221394\n",
            "QA 3360624\n",
            "Marketing 3158450\n",
            "Shakespeare 22\n",
            "GUTENBERG 99\n",
            "WILLIAM 115\n",
            "WORLD 98\n",
            "COLLEGE 98\n",
            "When 393\n",
            "Lord 341\n",
            "Library 2\n",
            "Top 15 words:\n",
            "the 13671\n",
            "and 12619\n",
            "of 9287\n",
            "to 8897\n",
            "i 8556\n",
            "a 6536\n",
            "my 5680\n",
            "in 5133\n",
            "that 4954\n",
            "you 4798\n",
            "is 4072\n",
            "with 3769\n",
            "for 3727\n",
            "his 3521\n",
            "not 3447\n",
            "\n",
            "Least 15 words:\n",
            "\"alas, 1\n",
            "\"as-is\". 1\n",
            "\"break 1\n",
            "\"brutus\" 1\n",
            "\"caesar\"? 1\n",
            "\"caesar, 1\n",
            "\"darest 1\n",
            "\"defect\" 1\n",
            "\"do 1\n",
            "\"give 1\n",
            "\"havoc!\" 1\n",
            "\"help 1\n",
            "\"help, 1\n",
            "\"liberty, 1\n",
            "\"lo, 1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PySparkProblemSolving\").getOrCreate()\n",
        "\n",
        "# 1. Count odd and even numbers\n",
        "integers_rdd = spark.sparkContext.textFile(\"integer.txt\")\n",
        "\n",
        "# Convert each line to an integer\n",
        "integers_rdd = integers_rdd.map(lambda x: int(x))\n",
        "\n",
        "# Filter for odd and even numbers\n",
        "odd_numbers = integers_rdd.filter(lambda x: x % 2 == 1)\n",
        "even_numbers = integers_rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Count the number of odd and even numbers\n",
        "odd_count = odd_numbers.count()\n",
        "even_count = even_numbers.count()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Odd numbers count:\", odd_count)\n",
        "print(\"Even numbers count:\", even_count)\n",
        "\n",
        "# 2. Calculate salary sum per department\n",
        "salary_rdd = spark.sparkContext.textFile(\"salary.txt\")\n",
        "\n",
        "# Split each line, extract department and salary, convert salary to integer\n",
        "salary_rdd = salary_rdd.map(lambda x: (x.split()[0], int(x.split()[1])))\n",
        "\n",
        "# Group by department and sum up the salaries\n",
        "salary_sum_per_dept = salary_rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Collect the results and print\n",
        "results = salary_sum_per_dept.collect()\n",
        "for dept, salary_sum in results:\n",
        "    print(dept, salary_sum)\n",
        "\n",
        "# 3. Implement MapReduce to count word occurrences\n",
        "shakespeare_rdd = spark.sparkContext.textFile(\"shakespeare-1.txt\")\n",
        "\n",
        "# Split each line into words, flatten, filter, map and reduce\n",
        "word_counts = shakespeare_rdd.flatMap(lambda line: line.split()) \\\n",
        "    .filter(lambda word: word in [\"Shakespeare\", \"When\", \"Lord\", \"Library\", \"GUTENBERG\", \"WILLIAM\", \"COLLEGE\", \"WORLD\"]) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect the results and print them\n",
        "results = word_counts.collect()\n",
        "for word, count in results:\n",
        "    print(word, count)\n",
        "\n",
        "# 4. Calculate top 15 and least 15 words\n",
        "shakespeare_rdd = spark.sparkContext.textFile(\"shakespeare-1.txt\")\n",
        "\n",
        "# Split, flatten, filter, convert to lowercase, map and reduce\n",
        "word_counts_sort = shakespeare_rdd.flatMap(lambda line: line.split()) \\\n",
        "    .filter(lambda word: word != '') \\\n",
        "    .map(lambda word: word.lower()) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Sort in descending order and take top 15\n",
        "top_15_words = word_counts_sort.sortBy(lambda x: x[1], ascending=False).take(15)\n",
        "\n",
        "# Sort in ascending order and take least 15 (considering words with same count)\n",
        "least_15_words = word_counts_sort.sortBy(lambda x: (x[1], x[0])).take(15)\n",
        "\n",
        "# Print the results\n",
        "print(\"Top 15 words:\")\n",
        "for word, count in top_15_words:\n",
        "    print(word, count)\n",
        "\n",
        "print(\"\\nLeast 15 words:\")\n",
        "for word, count in least_15_words:\n",
        "    print(word, count)\n",
        "\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
